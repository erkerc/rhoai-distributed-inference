apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
    opendatahub.io/runtime-version: v0.9.1.0
    openshift.io/display-name: vLLM Multi-Node ServingRuntime for KServe
  labels:
    opendatahub.io/dashboard: 'false'
  name: vllm-multinode-runtime
spec:
  annotations:
    opendatahub.io/kserve-runtime: vllm
    prometheus.io/path: /metrics
    prometheus.io/port: '8080'
  containers:
    - resources:
        limits:
          cpu: '6'
          memory: 16Gi
        requests:
          cpu: '2'
          memory: 8Gi
      readinessProbe:
        exec:
          command:
            - bash
            - '-c'
            - |
              # Check model health 
              health_check=$(curl -o /dev/null -s -w "%{http_code}\n" http://localhost:8080/health)
              if [[ ${health_check} != 200 ]]; then
                echo "Unhealthy - vLLM Runtime Health Check failed." 
                exit 1
              fi
        failureThreshold: 2
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 15
      name: kserve-container
      command:
        - bash
        - '-c'
        - |
          export MODEL_NAME=${MODEL_DIR}
          CMD="python3 -m vllm.entrypoints.openai.api_server \
              --distributed-executor-backend ray \
              --model=${MODEL_NAME} \
              --tensor-parallel-size=${TENSOR_PARALLEL_SIZE} \
              --pipeline-parallel-size=${PIPELINE_PARALLEL_SIZE} $0 $@"
          echo "*MultiNode VLLM Runtime Command*"
          echo "$CMD"

          echo ""
          echo "Ray Start"
          ray start --head --disable-usage-stats --include-dashboard false 
          # wait for other node to join
          until [[ $(ray status --address ${RAY_ADDRESS} | grep -c node_) -eq ${PIPELINE_PARALLEL_SIZE} ]]; do
            echo "Waiting..."
            sleep 1
          done
          ray status --address ${RAY_ADDRESS}

          exec $CMD
      livenessProbe:
        exec:
          command:
            - bash
            - '-c'
            - |
              # Check if the registered ray nodes count is greater or the same than PIPELINE_PARALLEL_SIZE
              registered_node_count=$(ray status --address ${RAY_ADDRESS} | grep -c node_)
              if [[ ! ${registered_node_count} -ge "${PIPELINE_PARALLEL_SIZE}" ]]; then
                echo "Unhealthy - Registered nodes count (${registered_node_count}) must not be less PIPELINE_PARALLEL_SIZE (${PIPELINE_PARALLEL_SIZE})."
                exit 1
              fi     

              # Check model health
              health_check=$(curl -o /dev/null -s -w "%{http_code}\n" http://localhost:8080/health)
              if [[ ${health_check} != 200 ]]; then
                echo "Unhealthy - vLLM Runtime Health Check failed." 
                exit 1
              fi
        failureThreshold: 2
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 15
      env:
        - name: RAY_USE_TLS
          value: '1'
        - name: RAY_TLS_SERVER_CERT
          value: /etc/ray/tls/tls.pem
        - name: RAY_TLS_SERVER_KEY
          value: /etc/ray/tls/tls.pem
        - name: RAY_TLS_CA_CERT
          value: /etc/ray/tls/ca.crt
        - name: RAY_PORT
          value: '6379'
        - name: RAY_ADDRESS
          value: '127.0.0.1:6379'
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: VLLM_NO_USAGE_STATS
          value: '1'
        - name: HOME
          value: /tmp
        - name: HF_HOME
          value: /tmp/hf_home
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
      startupProbe:
        exec:
          command:
            - bash
            - '-c'
            - |
              # This need when head node have issues and restarted.
              # It will wait for new worker node.                     
              registered_node_count=$(ray status --address ${RAY_ADDRESS} | grep -c node_)
              if [[ ! ${registered_node_count} -ge "${PIPELINE_PARALLEL_SIZE}" ]]; then
                echo "Unhealthy - Registered nodes count (${registered_node_count}) must not be less PIPELINE_PARALLEL_SIZE (${PIPELINE_PARALLEL_SIZE})."
                exit 1
              fi

              # Double check to make sure Model is ready to serve.
              for i in 1 2; do                    
                # Check model health
                health_check=$(curl -o /dev/null -s -w "%{http_code}\n" http://localhost:8080/health)
                if [[ ${health_check} != 200 ]]; then
                  echo "Unhealthy - vLLM Runtime Health Check failed." 
                  exit 1
                fi
              done
        failureThreshold: 40
        initialDelaySeconds: 20
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 30
      volumeMounts:
        - mountPath: /dev/shm
          name: shm
      image: 'registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:094db84a1da5e8a575d0c9eade114fa30f4a2061064a338e3e032f3578f8082a'
      args:
        - '--served-model-name={{.Name}}'
        - '--port=8080'
  multiModel: false
  supportedModelFormats:
    - autoSelect: true
      name: vLLM
      priority: 2
  volumes:
    - emptyDir:
        medium: Memory
        sizeLimit: 12Gi
      name: shm
  workerSpec:
    containers:
      - command:
          - bash
          - '-c'
          - |
            export RAY_HEAD_ADDRESS="${HEAD_SVC}.${POD_NAMESPACE}.svc.cluster.local:6379"

            SECONDS=0

            while true; do
              if (( SECONDS <= 240 )); then
                if ray health-check --address "${RAY_HEAD_ADDRESS}" > /dev/null 2>&1; then
                  echo "Global Control Service (GCS) is ready."
                  break
                fi
                echo "$SECONDS seconds elapsed: Waiting for Global Control Service (GCS) to be ready."
              else
                if ray health-check --address "${RAY_HEAD_ADDRESS}"; then
                  echo "Global Control Service (GCS) is ready. Any error messages above can be safely ignored."
                  break
                fi
                echo "$SECONDS seconds elapsed: Still waiting for Global Control Service (GCS) to be ready."
                echo "For troubleshooting, refer to the FAQ at https://docs.ray.io/en/master/cluster/kubernetes/troubleshooting/troubleshooting.html#kuberay-troubleshootin-guides"
              fi

              sleep 5
            done

            echo "Attempting to connect to Ray cluster at $RAY_HEAD_ADDRESS ..."
            ray start --address="${RAY_HEAD_ADDRESS}" --block
        env:
          - name: RAY_USE_TLS
            value: '1'
          - name: RAY_TLS_SERVER_CERT
            value: /etc/ray/tls/tls.pem
          - name: RAY_TLS_SERVER_KEY
            value: /etc/ray/tls/tls.pem
          - name: RAY_TLS_CA_CERT
            value: /etc/ray/tls/ca.crt
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
        image: 'registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:094db84a1da5e8a575d0c9eade114fa30f4a2061064a338e3e032f3578f8082a'
        livenessProbe:
          exec:
            command:
              - bash
              - '-c'
              - |
                # Check if the registered nodes count matches PIPELINE_PARALLEL_SIZE
                registered_node_count=$(ray status --address ${HEAD_SVC}.${POD_NAMESPACE}.svc.cluster.local:6379 | grep -c node_)
                if [[ ! ${registered_node_count} -ge "${PIPELINE_PARALLEL_SIZE}" ]]; then
                  echo "Unhealthy - Registered nodes count (${registered_node_count}) must not be less PIPELINE_PARALLEL_SIZE (${PIPELINE_PARALLEL_SIZE})."
                  exit 1
                fi
          failureThreshold: 2
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 15
        name: worker-container
        resources:
          limits:
            cpu: '8'
            memory: 16Gi
          requests:
            cpu: '4'
            memory: 8Gi
        startupProbe:
          exec:
            command:
              - /bin/sh
              - '-c'
              - |
                registered_node_count=$(ray status --address ${HEAD_SVC}.${POD_NAMESPACE}.svc.cluster.local:6379 | grep -c node_)
                if [[ ! ${registered_node_count} -ge "${PIPELINE_PARALLEL_SIZE}" ]]; then
                  echo "Unhealthy - Registered nodes count (${registered_node_count}) must not be less PIPELINE_PARALLEL_SIZE (${PIPELINE_PARALLEL_SIZE})."
                  exit 1
                fi  

                # Double check to make sure Model is ready to serve.
                for i in 1 2; do
                  # Check model health
                  model_health_check=$(curl -s ${HEAD_SVC}.${POD_NAMESPACE}.svc.cluster.local:8080/v1/models | grep -q '"object":"model"' && echo true || echo false)
                  if [[ "${model_health_check}" == "false" ]]; then
                    echo "Unhealthy - vLLM Runtime Health Check failed."
                    exit 1
                  fi
                  sleep 10
                done
          failureThreshold: 40
          initialDelaySeconds: 20
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 30
        volumeMounts:
          - mountPath: /dev/shm
            name: shm
    pipelineParallelSize: 2
    tensorParallelSize: 1
    nodeSelector:
      node.kubernetes.io/instance-type: g6.2xlarge
    tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        value: "true"
    volumes:
      - emptyDir:
          medium: Memory
          sizeLimit: 12Gi
        name: shm
